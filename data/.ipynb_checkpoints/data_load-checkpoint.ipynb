{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1665"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "with open('./preprocess/dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 85\n",
    "EVENT_TYPE = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195197 300\n",
      "Found 195197 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open('../../../../../../实验室/word2vec/sgns.weibo.bigram-char') as f:\n",
    "    lines = f.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    print(lines[0])\n",
    "    \n",
    "    for line in lines[1:]:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 6000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1665"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "for piece in dataset:\n",
    "    texts.append(piece['sentence_words'])\n",
    "    \n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014 年 1 月 7 日 广州 番禺市 桥街 兴泰路 商铺 火灾 ， 从化 女子 烧死 ！'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6476 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "we = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('消灭', 5305), ('卢氏', 5696), ('广大群众', 3838), ('茂密', 4585), ('国际', 786)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_index.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('双语', 3724)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_index.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 300), (1665, 85))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape, we.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,  567,  133,   38,    9,   66,    5,  805, 2996, 2997,\n",
       "       2998, 1140,   28,    1, 2999,  628, 3000, 1440], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014 年 1 月 7 日 广州 番禺市 桥街 兴泰路 商铺 火灾 ， 从化 女子 烧死 ！'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['sentence_words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(567, 133, 38, 9, 3000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['2014'], word_index['年'], word_index['1'], word_index['月'], word_index['烧死']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedding_matrix[3000] == embeddings_index['烧死']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./we_embedding_matrix_{}.npy'.format(embedding_matrix.shape), embedding_matrix)\n",
    "np.save('./we_{}.npy'.format(we.shape), we)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we_word_index.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(word_index, 'we_word_index.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Position: (1665, 85, 85, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1665"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./preprocess/dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we = np.load('./we_(1665, 85).npy')\n",
    "we.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6476"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = joblib.load('./we_word_index.pkl')\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,  567,  133,   38,    9,   66,    5,  805, 2996, 2997,\n",
       "       2998, 1140,   28,    1, 2999,  628, 3000, 1440], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '2014年1月7日 广州番禺市桥街兴泰路 商铺 火灾 ， 从化女子 烧死 ！',\n",
       " 'sentence_words': '2014 年 1 月 7 日 广州 番禺市 桥街 兴泰路 商铺 火灾 ， 从化 女子 烧死 ！',\n",
       " 'triggers': [{'event': 'emergency',\n",
       "   'event_arguments': '商铺',\n",
       "   'event_trigger': '火灾',\n",
       "   'index_event': 1,\n",
       "   'index_event_arguments': 10,\n",
       "   'index_event_trigger': 11}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(567, 28, 1140)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['2014'], word_index['火灾'], word_index['商铺']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_distance_arr(word, we_arr):\n",
    "    assert len(we_arr) == MAX_SEQUENCE_LENGTH\n",
    "    arr = np.arange(MAX_SEQUENCE_LENGTH)\n",
    "    \n",
    "    value = word_index[word]\n",
    "    index = np.where(we_arr==value)[0]\n",
    "    if len(index) != 0:\n",
    "        index = index[0]\n",
    "        \n",
    "        arr = np.abs(arr - index)\n",
    "        arr[np.where(we_arr==0)[0]] = -1\n",
    "        \n",
    "        return index, arr\n",
    "    else:\n",
    "        # 不可达\n",
    "        return -1, np.zeros(MAX_SEQUENCE_LENGTH) - 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0,  1,  2,  3,  4,  5]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_distance_arr('火灾',we[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85, 85, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = np.zeros((len(dataset), MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH, 2))\n",
    "position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, piece in enumerate(dataset):\n",
    "    triggers = piece['triggers']\n",
    "    for trigger in triggers:\n",
    "        # trigger词\n",
    "        t_word = trigger['event_trigger']\n",
    "        trigger_index, trigger_arr = cal_distance_arr(t_word, we[i])\n",
    "\n",
    "        # arguments词\n",
    "        try:\n",
    "            a_word = trigger['event_arguments']\n",
    "            argument_index, argument_arr = cal_distance_arr(a_word, we[i])\n",
    "        except:\n",
    "            argument_arr = np.zeros(MAX_SEQUENCE_LENGTH) - 1\n",
    "\n",
    "        if trigger_index == -1:\n",
    "            continue\n",
    "\n",
    "        position[i][trigger_index] = np.concatenate(\n",
    "            [trigger_arr[:, None], argument_arr[:, None]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([83]),)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(we[-1]==word_index['化验'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '昨晚 ， 当地卫生监督所 疾控中心 等部门正在 提取 相关样本 进行 化验 。',\n",
       " 'sentence_words': '昨晚 ， 当地 卫生 监督 所 疾控中心 等 部门 正在 提取 相关 样本 进行 化验 。',\n",
       " 'triggers': [{'event': 'operation',\n",
       "   'event_trigger': '提取',\n",
       "   'index_event': 3,\n",
       "   'index_event_trigger': 10},\n",
       "  {'event': 'operation',\n",
       "   'event_trigger': '化验',\n",
       "   'index_event': 3,\n",
       "   'index_event_trigger': 14}]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# position[-1][83]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./position_{}.npy'.format(position.shape), position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical: (1665, 85, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6476"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = joblib.load('./we_word_index.pkl')\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we = np.load('./we_(1665, 85).npy')\n",
    "we.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_index(word, we_arr):\n",
    "    assert len(we_arr) == MAX_SEQUENCE_LENGTH\n",
    "    \n",
    "    value = word_index[word]\n",
    "    index = np.where(we_arr==value)[0]\n",
    "    if len(index) != 0:\n",
    "        index = index[0]\n",
    "        return index\n",
    "    else:\n",
    "        # 不可达\n",
    "        return -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context_value(index, we_arr):\n",
    "    if index == -1:\n",
    "        return 0, 0, 0\n",
    "    \n",
    "    curr = we_arr[index]\n",
    "    try:\n",
    "        left = we_arr[index-1]\n",
    "    except:\n",
    "        left = 0\n",
    "    try:\n",
    "        right = we_arr[index+1]\n",
    "    except:\n",
    "        right = 0\n",
    "\n",
    "    return left, curr, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79, 28)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_index('火灾', we[0]), word_index['火灾']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,  567,  133,   38,    9,   66,    5,  805, 2996, 2997,\n",
       "       2998, 1140,   28,    1, 2999,  628, 3000, 1440], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '2014年1月7日 广州番禺市桥街兴泰路 商铺 火灾 ， 从化女子 烧死 ！',\n",
       " 'sentence_words': '2014 年 1 月 7 日 广州 番禺市 桥街 兴泰路 商铺 火灾 ， 从化 女子 烧死 ！',\n",
       " 'triggers': [{'event': 'emergency',\n",
       "   'event_arguments': '商铺',\n",
       "   'event_trigger': '火灾',\n",
       "   'index_event': 1,\n",
       "   'index_event_arguments': 10,\n",
       "   'index_event_trigger': 11}]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical = np.zeros((len(dataset), MAX_SEQUENCE_LENGTH, 6))\n",
    "lexical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, piece in enumerate(dataset):\n",
    "    triggers = piece['triggers']\n",
    "    for trigger in triggers:\n",
    "        # trigger词\n",
    "        t_word = trigger['event_trigger']\n",
    "        trigger_index = find_index(t_word, we[i])\n",
    "\n",
    "        # arguments词\n",
    "        try:\n",
    "            a_word = trigger['event_arguments']\n",
    "            argument_index = find_index(a_word, we[i])\n",
    "        except:\n",
    "            argument_index = -1\n",
    "            \n",
    "        if trigger_index == -1:\n",
    "            continue\n",
    "        \n",
    "        lexical[i][trigger_index][:3] = get_context_value(trigger_index, we[i])\n",
    "        lexical[i][trigger_index][3:] = get_context_value(argument_index, we[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 1140)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['火灾'], word_index['商铺']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.14000000e+03,   2.80000000e+01,   1.00000000e+00,\n",
       "         2.99800000e+03,   1.14000000e+03,   2.80000000e+01])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical[0][79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,  567,  133,   38,    9,   66,    5,  805, 2996, 2997,\n",
       "       2998, 1140,   28,    1, 2999,  628, 3000, 1440], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./lexical_{}.npy'.format(lexical.shape), lexical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical = np.load('./lexical_(1665, 85, 6).npy')\n",
    "lexical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '广州番禺警方 媒体 通报 ： 1月7日晚21时40分 警方 群众 报警 ， 称 市桥街兴泰路 商铺 冒出 浓烟 。',\n",
       " 'sentence_words': '广州 番禺 警方 媒体 通报 ： 1 月 7 日晚 21 时 40 分 警方 群众 报警 ， 称 市桥 街兴泰路 商铺 冒 出 浓烟 。',\n",
       " 'triggers': [{'event': 'statement',\n",
       "   'event_trigger': '称',\n",
       "   'index_event': 6,\n",
       "   'index_event_trigger': 18},\n",
       "  {'event': 'statement',\n",
       "   'event_trigger': '通报',\n",
       "   'index_event': 6,\n",
       "   'index_event_trigger': 4},\n",
       "  {'event': 'operation',\n",
       "   'event_trigger': '报警',\n",
       "   'index_event': 3,\n",
       "   'index_event_trigger': 16}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 429, 157)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['称'],word_index['通报'],word_index['报警']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 63, 75)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_index('称', we[1]), find_index('通报', we[1]), find_index('报警', we[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,  805, 1932,   68,  295,  429,  116,   38,\n",
       "          9,   66,  245,  162,    7,  156,   15,   68,  170,  157,    1,\n",
       "         57, 3001, 3002, 1140,  695,  186,  296,    2], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 170.,  157.,    1.,    0.,    0.,    0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexical[1][75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用于DynamicPooling：Mask (1665, 85, 3, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6476"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = joblib.load('./we_word_index.pkl')\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we = np.load('./we_(1665, 85).npy')\n",
    "we.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_index(word, we_arr):\n",
    "    assert len(we_arr) == MAX_SEQUENCE_LENGTH\n",
    "    \n",
    "    value = word_index[word]\n",
    "    index = np.where(we_arr==value)[0]\n",
    "    if len(index) != 0:\n",
    "        index = index[0]\n",
    "        return index\n",
    "    else:\n",
    "        # 不可达\n",
    "        return -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_mask_arr(t_index, a_index):\n",
    "    mask_arr = np.ones((3, MAX_SEQUENCE_LENGTH))\n",
    "    \n",
    "    partition = [t_index, a_index]\n",
    "    partition.sort()\n",
    "    low, high = partition\n",
    "    low = 0 if low < 0 else low\n",
    "    \n",
    "    mask_arr[0][0:low] = 0\n",
    "    mask_arr[1][low:high] = 0\n",
    "    mask_arr[2][high:] = 0\n",
    "    \n",
    "    return mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85, 3, 85)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.ones((len(dataset), MAX_SEQUENCE_LENGTH, 3, MAX_SEQUENCE_LENGTH))\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, piece in enumerate(dataset):\n",
    "    triggers = piece['triggers']\n",
    "    for trigger in triggers:\n",
    "        # trigger词\n",
    "        t_word = trigger['event_trigger']\n",
    "        trigger_index = find_index(t_word, we[i])\n",
    "\n",
    "        # arguments词\n",
    "        try:\n",
    "            a_word = trigger['event_arguments']\n",
    "            argument_index = find_index(a_word, we[i])\n",
    "        except:\n",
    "            argument_index = -1\n",
    "            \n",
    "        if trigger_index == -1:\n",
    "            continue\n",
    "        \n",
    "        mask[i][trigger_index] = gen_mask_arr(trigger_index, argument_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./mask_{}.npy'.format(mask.shape), mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '广州番禺警方 媒体 通报 ： 1月7日晚21时40分 警方 群众 报警 ， 称 市桥街兴泰路 商铺 冒出 浓烟 。',\n",
       " 'sentence_words': '广州 番禺 警方 媒体 通报 ： 1 月 7 日晚 21 时 40 分 警方 群众 报警 ， 称 市桥 街兴泰路 商铺 冒 出 浓烟 。',\n",
       " 'triggers': [{'event': 'statement',\n",
       "   'event_trigger': '称',\n",
       "   'index_event': 6,\n",
       "   'index_event_trigger': 18},\n",
       "  {'event': 'statement',\n",
       "   'event_trigger': '通报',\n",
       "   'index_event': 6,\n",
       "   'index_event_trigger': 4},\n",
       "  {'event': 'operation',\n",
       "   'event_trigger': '报警',\n",
       "   'index_event': 3,\n",
       "   'index_event_trigger': 16}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 429, 157)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['称'],word_index['通报'],word_index['报警']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 63, 75)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_index('称', we[1]), find_index('通报', we[1]), find_index('报警', we[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,  805, 1932,   68,  295,  429,  116,   38,\n",
       "          9,   66,  245,  162,    7,  156,   15,   68,  170,  157,    1,\n",
       "         57, 3001, 3002, 1140,  695,  186,  296,    2], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[1][75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '2014年1月7日 广州番禺市桥街兴泰路 商铺 火灾 ， 从化女子 烧死 ！',\n",
       " 'sentence_words': '2014 年 1 月 7 日 广州 番禺市 桥街 兴泰路 商铺 火灾 ， 从化 女子 烧死 ！',\n",
       " 'triggers': [{'event': 'emergency',\n",
       "   'event_arguments': '商铺',\n",
       "   'event_trigger': '火灾',\n",
       "   'index_event': 1,\n",
       "   'index_event_arguments': 10,\n",
       "   'index_event_trigger': 11}]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1140, 28)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['商铺'],word_index['火灾']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 79)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_index('商铺', we[0]), find_index('火灾', we[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,  567,  133,   38,    9,   66,    5,  805, 2996, 2997,\n",
       "       2998, 1140,   28,    1, 2999,  628, 3000, 1440], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         0.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[0][79]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label: (1665, 85, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6476"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = joblib.load('./we_word_index.pkl')\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we = np.load('./we_(1665, 85).npy')\n",
    "we.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_index(word, we_arr):\n",
    "    assert len(we_arr) == MAX_SEQUENCE_LENGTH\n",
    "    \n",
    "    value = word_index[word]\n",
    "    index = np.where(we_arr==value)[0]\n",
    "    if len(index) != 0:\n",
    "        index = index[0]\n",
    "        return index\n",
    "    else:\n",
    "        # 不可达\n",
    "        return -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 1,\n",
       " 'emergency': 2,\n",
       " 'movement': 3,\n",
       " 'operation': 4,\n",
       " 'perception': 5,\n",
       " 'stateChange': 6,\n",
       " 'statement': 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_type = dict()\n",
    "\n",
    "for sent in dataset:\n",
    "    for trigger in sent['triggers']:\n",
    "        t = trigger['event']\n",
    "        if t not in event_type.keys():\n",
    "            event_type[t] = 1\n",
    "        else:\n",
    "            event_type[t] += 1\n",
    "\n",
    "event_type\n",
    "\n",
    "events = list(event_type.keys())\n",
    "events.sort()\n",
    "\n",
    "event2index = dict(zip(events, [i+1 for i in range(len(events))]))\n",
    "event2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '2014年1月7日 广州番禺市桥街兴泰路 商铺 火灾 ， 从化女子 烧死 ！',\n",
       " 'sentence_words': '2014 年 1 月 7 日 广州 番禺市 桥街 兴泰路 商铺 火灾 ， 从化 女子 烧死 ！',\n",
       " 'triggers': [{'event': 'emergency',\n",
       "   'event_arguments': '商铺',\n",
       "   'event_trigger': '火灾',\n",
       "   'index_event': 1,\n",
       "   'index_event_arguments': 10,\n",
       "   'index_event_trigger': 11}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label = np.zeros((len(dataset), MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "for i, piece in enumerate(dataset):    \n",
    "    for trigger in piece['triggers']:\n",
    "        j = find_index(trigger['event_trigger'], we[i])\n",
    "        t = event2index[trigger['event']]\n",
    "        label[i][j] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1665, 85, 8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = to_categorical(label)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./label_{}.npy'.format(label.shape), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '广州番禺警方 媒体 通报 ： 1月7日晚21时40分 警方 群众 报警 ， 称 市桥街兴泰路 商铺 冒出 浓烟 。',\n",
       " 'sentence_words': '广州 番禺 警方 媒体 通报 ： 1 月 7 日晚 21 时 40 分 警方 群众 报警 ， 称 市桥 街兴泰路 商铺 冒 出 浓烟 。',\n",
       " 'triggers': [{'event': 'statement',\n",
       "   'event_trigger': '称',\n",
       "   'index_event': 6,\n",
       "   'index_event_trigger': 18},\n",
       "  {'event': 'statement',\n",
       "   'event_trigger': '通报',\n",
       "   'index_event': 6,\n",
       "   'index_event_trigger': 4},\n",
       "  {'event': 'operation',\n",
       "   'event_trigger': '报警',\n",
       "   'index_event': 3,\n",
       "   'index_event_trigger': 16}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 429, 157)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['称'],word_index['通报'],word_index['报警']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 63, 75)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_index('称', we[1]), find_index('通报', we[1]), find_index('报警', we[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,  805, 1932,   68,  295,  429,  116,   38,\n",
       "          9,   66,  245,  162,    7,  156,   15,   68,  170,  157,    1,\n",
       "         57, 3001, 3002, 1140,  695,  186,  296,    2], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event2index['statement'], event2index['operation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[1][75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '2014年1月7日 广州番禺市桥街兴泰路 商铺 火灾 ， 从化女子 烧死 ！',\n",
       " 'sentence_words': '2014 年 1 月 7 日 广州 番禺市 桥街 兴泰路 商铺 火灾 ， 从化 女子 烧死 ！',\n",
       " 'triggers': [{'event': 'emergency',\n",
       "   'event_arguments': '商铺',\n",
       "   'event_trigger': '火灾',\n",
       "   'index_event': 1,\n",
       "   'index_event_arguments': 10,\n",
       "   'index_event_trigger': 11}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1140, 28)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['商铺'],word_index['火灾']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 79)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_index('商铺', we[0]), find_index('火灾', we[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event2index['emergency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,  567,  133,   38,    9,   66,    5,  805, 2996, 2997,\n",
       "       2998, 1140,   28,    1, 2999,  628, 3000, 1440], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "we[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0][79]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
